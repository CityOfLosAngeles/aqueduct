{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitch JHU data through various schema changes\n",
    "* Reshape\n",
    "* See what columns we need to derive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/rogerallen/1583593\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "    # Add some other ones we found applicable\n",
    "    'US Virgin Islands': 'VI', \n",
    "    'United States Virgin Islands': 'VI',\n",
    "    'Grand Princess': 'Grand Princess',\n",
    "    'Diamond Princess': 'Diamond Princess', \n",
    "    'From Diamond Princess': 'Diamond Princess', \n",
    "    'Diamond Princess cruise ship': 'Diamond Princess'\n",
    "}\n",
    "\n",
    "# reverse the dict\n",
    "abbrev_us_state = dict(map(reversed, us_state_abbrev.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre 2/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre214_cases_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_2019-ncov-Confirmed.csv\"\n",
    "pre214_deaths_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_2019-ncov-Deaths.csv\"\n",
    "pre214_recovered_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_2019-ncov-Recovered.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases1 = pd.read_csv(pre214_cases_url)\n",
    "deaths1 = pd.read_csv(pre214_deaths_url)\n",
    "recovered1 = pd.read_csv(pre214_recovered_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified helper function, since columns are datetime, will extract date portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_columns(df):\n",
    "    \"\"\"\n",
    "    quick helper function to parse columns into values\n",
    "    uses for pd.melt\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.str.split(' ').str[0]\n",
    "    columns = list(df.columns)\n",
    "    id_vars, dates = [], []\n",
    "\n",
    "    for c in columns:\n",
    "        if c.endswith(\"20\"):\n",
    "            dates.append(c)\n",
    "        else:\n",
    "            id_vars.append(c)\n",
    "    return id_vars, dates\n",
    "\n",
    "# Rename geography columns to be the same as future schemas\n",
    "def rename_geog_cols(df):\n",
    "    df.rename(columns = {'Country/Region':'Country_Region', \n",
    "                         'Province/State': 'Province_State', \n",
    "                         'Long': 'Lon'}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions we'll use to get totals\n",
    "# Calculate US State totals\n",
    "def us_state_totals(df):\n",
    "    \n",
    "    state_grouping_cols = ['Country_Region', 'state_abbrev', 'date']\n",
    "    \n",
    "    state_totals = df.groupby(state_grouping_cols).agg(\n",
    "        {'cases':'sum', 'recovered':'sum', 'deaths':'sum'})\n",
    "    \n",
    "    state_totals.rename(columns = {'cases': 'state_cases',\n",
    "                                  'recovered':'state_recovered', \n",
    "                                  'deaths': 'state_deaths'}, inplace = True)\n",
    "    \n",
    "    df = pd.merge(df, state_totals, on = state_grouping_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Calculate non-US Province_State totals\n",
    "def province_totals(df):\n",
    "    \n",
    "    province_grouping_cols = ['Country_Region', 'Province_State', 'date']\n",
    "\n",
    "    province_totals = df.groupby(province_grouping_cols).agg(\n",
    "        {'cases':'sum', 'recovered':'sum', 'deaths':'sum'})\n",
    "    \n",
    "    province_totals.rename(columns = {'cases': 'state_cases',\n",
    "                                  'recovered':'state_recovered', \n",
    "                                  'deaths': 'state_deaths'}, inplace = True)\n",
    "    \n",
    "    df = pd.merge(df, province_totals, on = province_grouping_cols) \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Calculate country totals\n",
    "def country_totals(df):\n",
    "    \n",
    "    country_grouping_cols = ['Country_Region', 'date']\n",
    "    \n",
    "    country_totals = df.groupby(country_grouping_cols).agg(\n",
    "        {'cases':'sum', 'recovered':'sum', 'deaths':'sum'})\n",
    "    \n",
    "    country_totals.rename(columns = {'cases': 'country_cases',\n",
    "                                  'recovered':'country_recovered', \n",
    "                                  'deaths': 'country_deaths'}, inplace = True)\n",
    "    \n",
    "    df = pd.merge(df, country_totals, on = country_grouping_cols) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars, dates = parse_columns(cases1)\n",
    "pre214_df = pd.melt(cases1, id_vars=id_vars, value_vars=dates, value_name=\"cases\", var_name=\"date\",\n",
    ")\n",
    "\n",
    "# melt deaths\n",
    "id_vars, dates = parse_columns(deaths1)\n",
    "deaths_df = pd.melt(deaths1, id_vars=id_vars, value_vars=dates, value_name=\"deaths\")\n",
    "\n",
    "# melt recovered\n",
    "id_vars, dates = parse_columns(recovered1)\n",
    "recovered_df = pd.melt(\n",
    "    recovered1, id_vars=id_vars, value_vars=dates, value_name=\"recovered\"\n",
    ")\n",
    "\n",
    "# join\n",
    "pre214_df[\"deaths\"] = deaths_df.deaths\n",
    "pre214_df[\"recovered\"] = recovered_df.recovered\n",
    "\n",
    "pre214_df['date'] = pd.to_datetime(pre214_df.date)\n",
    "\n",
    "part1 = rename_geog_cols(pre214_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "world1 = part1[part1.Country_Region != 'US'] \n",
    "us1 = part1[part1.Country_Region == 'US']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre 3/23\n",
    "### This is in 2 groups: 2/15-3/9 and 3/10-3/23, call parts 2 and 3\n",
    "* part 2 is county level...which need to be summed up to get state totals (subset and keep 2/15 - 3/9)\n",
    "* part 3 is state level (subset and keep 3/10-3/23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre323_cases_url= \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Confirmed_archived_0325.csv\"\n",
    "pre323_deaths_url= \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Deaths_archived_0325.csv\"\n",
    "pre323_recovered_url= \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Recovered_archived_0325.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases2 = pd.read_csv(pre323_cases_url)\n",
    "deaths2 = pd.read_csv(pre323_deaths_url)\n",
    "recovered2 = pd.read_csv(pre323_recovered_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars, dates = parse_columns(cases2)\n",
    "pre323_df = pd.melt(cases2, id_vars=id_vars, value_vars=dates, value_name=\"cases\", var_name=\"date\",\n",
    ")\n",
    "\n",
    "# melt deaths\n",
    "id_vars, dates = parse_columns(deaths2)\n",
    "deaths_df2 = pd.melt(deaths2, id_vars=id_vars, value_vars=dates, value_name=\"deaths\")\n",
    "\n",
    "# melt recovered\n",
    "id_vars, dates = parse_columns(recovered2)\n",
    "recovered_df2 = pd.melt(\n",
    "    recovered2, id_vars=id_vars, value_vars=dates, value_name=\"recovered\"\n",
    ")\n",
    "\n",
    "# join\n",
    "pre323_df[\"deaths\"] = deaths_df2.deaths\n",
    "pre323_df[\"recovered\"] = recovered_df2.recovered\n",
    "\n",
    "pre323_df['date'] = pd.to_datetime(pre323_df.date)\n",
    "\n",
    "part2 = rename_geog_cols(pre323_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset into part2 and part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start2 = '2/15/2020'\n",
    "end2 = '3/9/2020'\n",
    "\n",
    "start3 = '3/10/2020'\n",
    "end3 = '3/23/2020'\n",
    "\n",
    "world2 = part2[(part2.Country_Region != 'US') & (part2.date >= start2) & (part2.date <= end2)] \n",
    "us2 = part2[(part2.Country_Region == 'US') & (part2.date >= start2) & (part2.date <= end2)]  \n",
    "\n",
    "world3 = part2[(part2.Country_Region != 'US') & (part2.date >= start3) & (part2.date <= end3)] \n",
    "us3 = part2[(part2.Country_Region == 'US') & (part2.date >= start3) & (part2.date <= end3)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up each respective part with the right filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us2 has county-level data, but also state and country-level observations. Drop those.\n",
    "us2 = us2[(us2.Province_State.str.contains(',') == True) | \n",
    "              (us2.Province_State.str.contains('Princess') == True)]\n",
    "\n",
    "us2 = us2[us2.Province_State != 'US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us3 has state-level data, but also county and country-level observations. Drop those.\n",
    "us3 = us3[(us3.Province_State.str.contains(',') == False) | \n",
    "              (us3.Province_State.str.contains('Princess') == True)]\n",
    "\n",
    "us3 = us3[us3.Province_State != 'US']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append parts 1-3 together, and do some cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = us1.append(us2, sort = False)\n",
    "world = world1.append(world2, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state_abbrev column, with special case for the cruise ships\n",
    "county['state_abbrev'] = county.Province_State.str.split(', ', expand = True)[1]\n",
    "\n",
    "county['state_abbrev'] = county.apply(lambda row: row.Province_State if row.state_abbrev is None \n",
    "                                else row.state_abbrev, axis = 1)\n",
    "\n",
    "# Create an orig_county columns that stores county-level name. Use to merge later on.\n",
    "county['orig_county'] = county.Province_State\n",
    "\n",
    "# Let's remove \"county\" the name, since sometimes it's Los Angeles County, CA or Los Angeles, CA\n",
    "county['orig_county'] = county.orig_county.str.replace(' County,', ',')\n",
    "county.orig_county = county.orig_county.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state_abbrev column\n",
    "us3['state_abbrev'] = us3.Province_State.map(us_state_abbrev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get state and country totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = us_state_totals(county)\n",
    "county = country_totals(county)\n",
    "\n",
    "world = province_totals(world)\n",
    "world = country_totals(world)\n",
    "\n",
    "us3 = us_state_totals(us3)\n",
    "us3 = country_totals(us3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have state/country totals, set the cases, deaths, recovered values to 0 for 3/10-3/23\n",
    "for col in ['cases', 'deaths', 'recovered']:\n",
    "    us3[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the US data up to 3/23 together\n",
    "county = county.append(us3, sort = False)\n",
    "\n",
    "# Append all the US and world data together up to 3/23 (before most recent massive schema change)\n",
    "jhu1 = county.append(world, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of duplicates so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_cleaning(df):\n",
    "    \n",
    "    df = df.drop_duplicates(subset = ['Country_Region', 'Lat', 'Lon', 'state_abbrev',\n",
    "                                            'date', 'cases', 'deaths', 'recovered'])\n",
    "    \n",
    "    # If there are still duplicates, it's because JHU sometimes did multiple updates a day\n",
    "    # This is ok, we'll keep the higher values for cases, deaths, recovered. \n",
    "    for col in ['cases', 'deaths', 'recovered']:\n",
    "        df[col] = df.groupby(['Province_State', 'Country_Region', \n",
    "                              'Lat', 'Lon', 'date'])[col].transform('max').fillna(0).astype(int)\n",
    "\n",
    "    df = df.drop_duplicates(subset = ['Province_State', 'Country_Region',\n",
    "                                      'Lat', 'Lon', 'date', 'cases', 'deaths', 'recovered'], keep = 'last')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "jhu1 = some_cleaning(jhu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post 3/23 feature layer\n",
    "* The feature layer only has the current date's information\n",
    "* For 3/25 and 3/26, we have saved geojsons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer_url = \"https://services1.arcgis.com/0MSEUqKaxRlEPj5g/ArcGIS/rest/services/ncov_cases_US/FeatureServer/0/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=OBJECTID%2C+Province_State%2C+Country_Region%2C+Last_Update%2C+Lat%2C+Long_%2C+Confirmed%2C+Recovered%2C+Deaths%2C+Active%2C+Admin2%2C+FIPS%2C+Combined_Key%2C+Incident_Rate%2C+People_Tested&returnGeometry=true&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token=\"\n",
    "\n",
    "#cases326 = gpd.read_file(feature_layer_url)\n",
    "\n",
    "#cases326.to_file(driver = 'GeoJSON', filename = '../data/jhu_feature_layer_3_26_2020.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases325 = gpd.read_file('s3://public-health-dashboard/jhu_covid19/jhu_feature_layer_3_25_2020.geojson')\n",
    "cases326 = gpd.read_file('s3://public-health-dashboard/jhu_covid19/jhu_feature_layer_3_26_2020.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need help with Last Update column....it's displaying weird ESRI stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases325['date'] = '3/25/2020'\n",
    "cases326['date'] = '3/26/2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append what we have of JHU's new layer so far\n",
    "post323_df = cases325.append(cases326)\n",
    "\n",
    "post323_df['date'] = pd.to_datetime(post323_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jhu_post323_schema(df):\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns = {\"Long_\":\"Lon\", \n",
    "                        \"Confirmed\":\"cases\", \n",
    "                        \"Recovered\":\"recovered\", \n",
    "                        \"Deaths\":\"deaths\", \n",
    "                        \"Admin2\": \"County\"} , inplace = True)  \n",
    "    \n",
    "    \"\"\"\n",
    "    These are the geographic identifiers\n",
    "    Admin2 = County\n",
    "    Province_State = US State\n",
    "    Combined_Key = County, State, Country    \n",
    "    \"\"\"\n",
    "    df['state_abbrev'] = df.Province_State.map(us_state_abbrev)\n",
    "    df['orig_county'] = df.County + \", \" + df.state_abbrev\n",
    "    \n",
    "    # Remove the word \"County\" from orig_county. No difference between Los Angeles County, CA and Los Angeles, CA\n",
    "    df['orig_county'] = df.orig_county.str.replace(' County,', ',')\n",
    "    df.orig_county = df.orig_county.str.strip()\n",
    "    \n",
    "    # Now change the columns to match with previous schemas\n",
    "    # Province_State will now display county, state abbrev (Los Angeles, CA)\n",
    "    df.Province_State = df.orig_county\n",
    "    \n",
    "    # Add state and country totals (JHU only collecting US county data now, no more non-US country observations)\n",
    "    df = us_state_totals(df)\n",
    "    df = country_totals(df)\n",
    "    \n",
    "    # Drop columns\n",
    "    df = df.drop(columns = ['County', 'Active', 'OBJECTID', 'Last_Update'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "part4 = clean_jhu_post323_schema(post323_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have new columns FIPS and Combined_Key in part4. Apply that to jhu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS and Combined_Key for combined_df3 before appending\n",
    "fips_key_crosswalk = part4[['orig_county', 'FIPS', 'Combined_Key']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu2 = pd.merge(jhu1, fips_key_crosswalk, on = 'orig_county', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some who don't have FIPS in the US. There are a couple that need to be manually taken care of.\n",
    "fix_me = jhu2[(jhu2.Country_Region =='US')& (jhu2.FIPS.isna() & \n",
    "                                    (jhu2.Province_State.str.contains(',')))]\n",
    "\n",
    "jhu2 = jhu2[~jhu2.orig_county.isin(fix_me.orig_county)]\n",
    "\n",
    "\n",
    "fix_FIPS = {\n",
    "    'New York, NY': '36061',\n",
    "    'Jefferson Parish, LA': '22051', \n",
    "    'Washington, D.C.': '11001'\n",
    "}\n",
    "\n",
    "fix_combined_key = {\n",
    "    'New York, NY': 'New York City, New York, US',\n",
    "    'Jefferson Parish, LA': 'Jefferson, Louisiana, US',\n",
    "    'Washington, D.C.': 'District of Columbia,District of Columbia,US'\n",
    "}\n",
    "\n",
    "\n",
    "fix_me['FIPS'] = fix_me.orig_county.map(fix_FIPS)\n",
    "fix_me['Combined_Key'] = fix_me.orig_county.map(fix_combined_key)\n",
    "\n",
    "\n",
    "jhu3 = jhu2.append(fix_me, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates, which come from slightly different spellings of county names\n",
    "jhu3 = jhu3.drop_duplicates(subset = ['Country_Region', 'Province_State', 'FIPS', 'Lat', 'Lon', \n",
    "                                      'state_abbrev', 'date', \n",
    "                                      'cases', 'deaths', 'recovered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jhu3.to_parquet('../data/compiled_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
